---
title: "Presentation"
author: "Ethan Straub and Ronnie Delgado"
date: "2025-06-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load Packages
```{r}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(glmnet)
library(randomForest)
library(ggcorrplot)
library(mice)
library(xgboost)
library(dbarts)
library(BART)
library(knitr)
```


Ronnie Load Data
```{r}
train <- read.csv("C:/Users/Ronnie/Downloads/tbistaa556_training.csv")
validate <- read.csv("C:/Users/Ronnie/Downloads/tbistaa556_validate.csv")
```


Ethan Load Data
```{r}
train = read.csv("tbistaa556_training.csv")
validate = read.csv("tbistaa556_validate.csv")
```


Data Cleaning
```{r}
# reomving the died and subject id columns
train = train[,-1]
validate = validate[,-1]
train = train[,-36]
validate = validate[,-35]

# converting NA in the race column to unknown
indR = is.na(train$race7)
train$race7[indR] = 0
```


How many NAs per column?
```{r}
for(i in 1:ncol(xtrain)){
  cat("\nColumn ", names(xtrain)[i], ". Num_NAs:", sum(is.na(xtrain[,i])))
}
```

Making new Data Frame with strings instead of numeric indicators
```{r}
library(dplyr)
train1 = train
train1 = train1 %>% mutate(race7 = case_when(
  race7 == 0 ~ "Unknown",
  race7 == 1 ~ "Multi-race",
  race7 == 2 ~ "Pacific Islander",
  race7 == 3 ~ "American Indian",
  race7 == 4 ~ "Asian",
  race7 == 5 ~ "Black",
  race7 == 6 ~ "White"))

train1 = train1 %>% mutate(ethnic3 = case_when(
  ethnic3 == 0 ~ "Unknown",
  ethnic3 == 1 ~ "Hispanic",
  ethnic3 == 2 ~ "Non-Hispanic"))


train1 = train1 %>% mutate(sex2 = case_when(
  sex2 == 0 ~ "Unknown",
  sex2 == 1 ~ "Female",
  sex2 == 2 ~ "Male"))

train1 = train1 %>% mutate(sig_other = case_when(
  sig_other == 0 ~ "Unknown",
  sig_other == 1 ~ "No Significant Other",
  sig_other == 2 ~ "Significant Other"))

train1 = train1 %>% mutate(tobacco = case_when(
  tobacco == 0 ~ "Missing",
  tobacco == 1 ~ "Yes",
  tobacco == 2 ~ "No"))

train1 = train1 %>% mutate(alcohol = case_when(
  alcohol == 0 ~ "Missing",
  alcohol == 1 ~ "Yes",
  alcohol == 2 ~ "No"))

train1 = train1 %>% mutate(drugs = case_when(
  drugs == 0 ~ "Missing",
  drugs == 1 ~ "Yes",
  drugs == 2 ~ "No"))

train1 = train1 %>% mutate(payertype = case_when(
  payertype == 1 ~ "Work Comp and Self Pay",
  payertype == 2 ~ "Medicaid",
  payertype == 3 ~ "Medicare",
  payertype == 4 ~ "VA and Indian Services",
  payertype == 5 ~ "Commercial"))

train1 = train1 %>% mutate(ptp1_yn = case_when(
  ptp1_yn == 1 ~ "Yes",
  ptp1_yn == 2 ~ "No",))

train1 = train1 %>% mutate(ptp2_yn = case_when(
  ptp2_yn == 1 ~ "Yes",
  ptp2_yn == 2 ~ "No",))

train1 = train1 %>% mutate(ptp0_yn = case_when(
  ptp0_yn == 1 ~ "Yes",
  ptp0_yn == 2 ~ "No",))

train1 = train1 %>% mutate(ed_yn = case_when(
  ed_yn == 0 ~ "Missing",
  ed_yn == 1 ~ "Yes",
  ed_yn == 2 ~ "No"))

train1 = train1 %>% mutate(icu = case_when(
  icu == 1 ~ "Yes",
  icu == 2 ~ "No"))

train1 = train1 %>% mutate(delirium = case_when(
  delirium == 0 ~ "Missing",
  delirium == 1 ~ "Yes",
  delirium == 2 ~ "No"))

train1 = train1 %>% mutate(agitated = case_when(
  agitated == 0 ~ "Missing",
  agitated == 1 ~ "Yes",
  agitated == 2 ~ "No"))

train1 = train1 %>% mutate(lethargic = case_when(
  lethargic == 0 ~ "Missing",
  lethargic == 1 ~ "Yes",
  lethargic == 2 ~ "No"))

train1 = train1 %>% mutate(comatose = case_when(
  comatose == 0 ~ "Missing",
  comatose == 1 ~ "Yes",
  comatose == 2 ~ "No"))

train1 = train1 %>% mutate(disoriented = case_when(
  disoriented == 0 ~ "Missing",
  disoriented == 1 ~ "Yes",
  disoriented == 2 ~ "No"))

train1 = train1 %>% mutate(dc_setting = case_when(
  dc_setting == 0 ~ "Died",
  dc_setting == 1 ~ "Unknown",
  dc_setting == 2 ~ "Hospice",
  dc_setting == 3 ~ "Other",
  dc_setting == 4 ~ "AMA",
  dc_setting == 5 ~ "IRF",
  dc_setting == 6 ~ "SNF",
  dc_setting == 7 ~ "Mental/Psych",
  dc_setting == 8 ~ "Home",))

train1 = train1 %>% mutate(prehosp = case_when(
  prehosp == 1 ~ "Yes",
  prehosp == 2 ~ "No"))

train1 = train1 %>% mutate(posthosp = case_when(
  posthosp == 1 ~ "Yes",
  posthosp == 2 ~ "No"))

train1 = train1 %>% mutate(ptp3_yn = case_when(
  ptp3_yn == 0 ~ "No",
  ptp3_yn == 1 ~ "Yes"))

```


Graphs of each predictor and response
```{r, warning = FALSE}
graphs = function(train, numUnique = 8){
  for(col in 1:ncol(train)){
  uvs = sort(unique(train[,col]))
  # skip continuous variables
  if(length(uvs) <= numUnique){
    next
  }
  colname = colnames(train)[col]
  print(ggplot(train, aes(x = .data[[colname]], y = as.factor(ptp3_yn), color = as.factor(ptp3_yn))) + 
    geom_jitter(size = 1, alpha = 0.5) + 
      labs(x = colname, y = "PTP Post") + theme(legend.position = "none"))
  }
}


graphs(train1, numUnique = 1)
```


Transforming predictors
```{r}
# performance summaries
train$summaryGCS = (train$gcs_min + train$gcs_max)/2
train$summaryADL = (train$adl_min + train$adl_max)/2
train$summaryMOB = (train$mobility_min + train$mobility_max)/2

# put ceiling on Percentages
train$PABEFZ = ifelse(train$PercentAboveBachelorsEducationForZip>1,train$PercentAboveBachelorsEducationForZip/100,train$PercentAboveBachelorsEducationForZip)
train$PAHSEFZ = ifelse(train$PercentAboveHighSchoolEducationForZip>1,train$PercentAboveHighSchoolEducationForZip/100,train$PercentAboveHighSchoolEducationForZip)

# log skewed right predictors
train$loglos_total = log(train$los_total+1)
train$logtbiS02 = log(train$tbiS02+1)
train$logtbiS06 = log(train$tbiS06+1)
train$logtbiS09 = log(train$tbiS09+1)
data = train
```


Creating variable importance graphs with LASSO and Random Forest
```{r}
# removing na values from train
trainClean = na.omit(train)

# fitting random forest
trainClean$ptp3_yn = as.factor(trainClean$ptp3_yn)
rf_fit <- randomForest(ptp3_yn ~ ., data = trainClean)


# plotting which predictors are the most important
data.frame(rf_fit$importance )%>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(MeanDecreaseGini)])) %>% 
  ggplot() +
  geom_point(aes(MeanDecreaseGini, variable))+
  labs(title = "Variable Importance")-> Rf_image

ggsave("Rf_image.png")

# Using model.matrix to create a matrix of all predictors suitable for lasso GLM 
X = model.matrix(ptp3_yn ~., data = trainClean)[,-36]
X = X[,-1]

y <- as.numeric(trainClean[,36])

# creating a vector of lambda values
lambda <- 10^seq(-4, -1, length.out = 100)

# creating lasso fit
lassofit = glmnet(X, y, alpha = 1, lambda = lambda, family = "binomial")


# doing 10 fold cv on lasso model for best lambda value
cv_lasso <- cv.glmnet(X, y, alpha = 1, lambda = lambda)


# Ploitting cv test mse and lambda values 
ggplot() +
  geom_line(aes(cv_lasso$lambda, cv_lasso$cvm)) +
  geom_point(aes(cv_lasso$lambda, cv_lasso$cvm)) +
  scale_x_log10()+
  labs(x = "Lambda", y = "CV Error", title = "CV Error Across Lambdas") -> lasso_image

ggsave("lasso_image.png")

cv_lasso$lambda.min
# the number of beta values which don't go to zero when we use optimal lambda value
cv_lasso$nzero[which.min(cv_lasso$cvm)]


# Gettinh coefficients at best lambda
best_lambda <- cv_lasso$lambda.min
best_model_coefs <- coef(cv_lasso, s = "lambda.min")



```



## Space for model selection results


```{r}
Allnames = c("ptp3_yn","age", "race7", "ethnic3", "sex2", "sig_other", "tobacco","alcohol", "drugs","MedianIncomeForZip",
"PercentAboveHighSchoolEducationForZip", "PercentAboveBachelorsEducationForZip", "payertype", "tbiS02","tbiS06", "tbiS09", "ptp1_yn", "ptp2_yn", "ptp0_yn", "ed_yn", "icu","delirium", "agitated", "lethargic","comatose", "disoriented", "gcs_min", "gcs_max", "adl_min", "adl_max", "mobility_min", "mobility_max", "los_total", "dc_setting", "prehosp", "posthosp", "summaryGCS","summaryADL", "summaryMOB", "loglos_total", "PABEFZ","PAHSEFZ", "logtbiS02", "logtbiS06", "logtbiS09")

names1 = c("ptp3_yn","age", "race7", "ethnic3", "sex2", "sig_other","tobacco", "alcohol", "drugs", "MedianIncomeForZip", "payertype", "ptp1_yn", "ptp2_yn", "ptp0_yn", "ed_yn", "icu", "delirium", "agitated", "lethargic","comatose", "disoriented","dc_setting", "prehosp", "posthosp", "gcs_min", "gcs_max", "adl_min", "adl_max", "mobility_min", "mobility_max", "loglos_total", "PABEFZ",   "PAHSEFZ", "logtbiS02", "logtbiS06", "logtbiS09")

names2 = c("ptp3_yn","age", "race7", "sex2", "sig_other","tobacco", "alcohol", 
"drugs", "MedianIncomeForZip", "payertype", "ptp1_yn", "ptp2_yn", "ptp0_yn", "ed_yn", "delirium", "dc_setting", "prehosp", "posthosp", "summaryADL", 
"mobility_min", "mobility_max", "loglos_total",                               "PAHSEFZ", "logtbiS02", "logtbiS06", "logtbiS09")

names3 = c("ptp3_yn","age", "race7", "sex2", "sig_other","tobacco", "alcohol", 
"drugs", "MedianIncomeForZip", "payertype","ptp1_yn", "ptp2_yn", "ptp0_yn", "ed_yn", "delirium", "disoriented","dc_setting", "prehosp", "posthosp", "summaryADL", "mobility_min", "mobility_max", "loglos_total",                     "PAHSEFZ", "PAHSEFZ", "logtbiS02", "logtbiS06", "logtbiS09")
```


Function for cross validation splits 
```{r}
kfoldData = function(impute = FALSE, colnames, data, seed = 556, k = 5, printImputer = FALSE){
  set.seed(seed)
  
  # shuffiling rows of data frame
  ind = sample(nrow(data))
  data = data[ind,]
  
  # selecting columns in colnames
  dta = data %>% select(all_of(colnames))
  
  # initializing list to return
  listReturn = list()
  
  # convert to factors
  for (col in names(dta)) {
    if(col == "ptp3_yn"){
      next
    }
    if (length(unique(dta[[col]])) < 8) {
      dta[[col]] = as.factor(dta[[col]])
    }
  }
  dta$dc_setting = as.factor(dta$dc_setting)
  mf = model.frame(~.-1, data = dta, na.action = na.pass)
  dta = model.matrix(~.-1, data = mf)
  dta = as.data.frame(dta)
  
  # If not imputing, remove rows with NA
  if(impute == FALSE){
    ind = complete.cases(dta)
    dta = dta[ind,]
  }
  
  # Splitting features and labels
  X = dta %>% select(-"ptp3_yn")
  Y = dta %>% select("ptp3_yn")
  
  # Number of rows per fold
  numPerFold = round(nrow(dta)/k)
  
  #loop through each split
  for(i in 1:k){
    
    # Get indices of train and test set for the ith iteration
    startInd = (i-1)*numPerFold+1
    endInd = ifelse(i == k, nrow(dta), i*numPerFold)
    testInd = startInd:endInd
    
    # split features and labels into train and test sets
    Xtest = X[testInd,]
    Xtrain = X[-testInd,]
    Ytest = Y[testInd,]
    Ytrain = Y[-testInd,]
    
    # imputing
    if(impute == TRUE){
      
      # imputing Xtrain
      imputeObj = mice(Xtrain, m = 1, seed = seed, printFlag = printImputer)
      #print(imputeObj$loggedEvents)
      Xtrain = complete(imputeObj, 1)
      # imputing Xtest using the already imputed Xtrain
      full = rbind(Xtrain, Xtest)
      fullImputeObj = mice(full, m = 1, maxit = 1, seed = seed, printFlag = printImputer)
      fullImputed = complete(fullImputeObj, 1)
      testStart = nrow(Xtrain)+1
      Xtest = fullImputed[testStart:nrow(fullImputed),]
    }
    # Append to the list that is returned
    listReturn[[4*i-3]] = Xtrain
    listReturn[[4*i-2]] = Ytrain
    listReturn[[4*i-1]] = Xtest
    listReturn[[4*i]] = Ytest
  }
  # naming elements in the list
  for(i in 1:k){
    start = 4*i-3
    end = start + 3
    names(listReturn)[start:end] = c(paste0("Xtrain",i), paste0("Ytrain",i),
                                     paste0("Xtest",i), paste0("Ytest",i))
  }
  return(listReturn)
}

CrossEntropyLoss = function(probabilities,truth){
  probabilities <- pmin(pmax(probabilities, 1e-15), 1 - 1e-15)
  return(-mean((truth*log(probabilities)) + (1-truth)*log(1-probabilities)))
}
```


Get splits for Imputed/not imputed for all 3 sets of predictors
```{r}
noImp1 = kfoldData(colnames = names1, data = data, impute = FALSE, k = 5)
noImp2 = kfoldData(colnames = names2, data = data, impute = FALSE, k = 5)
noImp3 = kfoldData(colnames = names3, data = data, impute = FALSE, k = 5)
Imp1 = kfoldData(colnames = names1, data = data, impute = TRUE, k = 5)
Imp2 = kfoldData(colnames = names2, data = data, impute = TRUE, k = 5)
Imp3 = kfoldData(colnames = names3, data = data, impute = TRUE, k = 5)
```




Lasso GLM Results
```{r}
LASSO = function(kfold_list, lambda = 0.00231013, seed = 556){

error_rates <- numeric(5)
cross_entropies_losses <- numeric(5)
true_positive_rates <- numeric(5)
true_negative_rates <- numeric(5)
all_predictions <- c()
all_truths <- c()

# looping through each kfold split
for (i in 1:5) {
  Xtrain <- as.matrix(kfold_list[[paste0("Xtrain", i)]])
  Ytrain <- as.vector(kfold_list[[paste0("Ytrain", i)]])
  Xtest <- as.matrix(kfold_list[[paste0("Xtest", i)]])
  Ytest <- as.vector(kfold_list[[paste0("Ytest", i)]])

  lasso_fit <- glmnet(Xtrain, Ytrain, alpha = 1, lambda = 0.00231013, family = binomial)
  
  pred_probs <- predict(lasso_fit, s = 0.00231013, newx = Xtest, type = "response")
  
  predictions <- ifelse(pred_probs > 0.5, 1, 0)
  
  error_rates[i] <- mean(predictions != Ytest)
  
  cross_entropies_losses[i] <- CrossEntropyLoss(pred_probs, Ytest)
  
  TP <- sum(predictions == 1 & Ytest == 1)
  TN <- sum(predictions == 0 & Ytest == 0)
  FP <- sum(predictions == 1 & Ytest == 0)
  FN <- sum(predictions == 0 & Ytest == 1)
  
  true_positive_rates[i] <- ifelse((TP + FN) > 0, TP / (TP + FN))
  true_negative_rates[i] <- ifelse((TN + FP) > 0, TN / (TN + FP))
}

 mean_error_rate <- mean(error_rates)
  mean_cross_entropy_loss <- mean(cross_entropies_losses)
  mean_tp_rate <- mean(true_positive_rates)
  mean_tn_rate <- mean(true_negative_rates)

return( kable(data.frame(mean_error_rate,mean_cross_entropy_loss,
      mean_tn_rate, mean_tp_rate), digits = 3)
    )


}

LASSO(noImp1)
LASSO(noImp2)
LASSO(noImp3)
LASSO(Imp1)
LASSO(Imp2)
LASSO(Imp3)

```

Random Forest Results
```{r}
RF = function(kfold_list, seed = 556){
  
error_rates <- numeric(5)
cross_entropies_losses <- numeric(5)
true_positive_rates <- numeric(5)
true_negative_rates <- numeric(5)

kfold_list <- kfoldData(impute = TRUE, colnames = names1, data = fulldata, k = 5)

for (i in 1:5) {
  Xtrain <- kfold_list[[paste0("Xtrain", i)]]
  Ytrain <- kfold_list[[paste0("Ytrain", i)]]
  Xtest <- kfold_list[[paste0("Xtest", i)]]
  Ytest <- kfold_list[[paste0("Ytest", i)]]
  
  train_df <- data.frame(Xtrain, ptp3_yn = as.factor(Ytrain))
  test_df <- data.frame(Xtest)
  Ytest <- as.factor(Ytest)
  
   fit <- randomForest(ptp3_yn ~ ., data = train_df, )
  
  pred_probs <- predict( fit, newdata = test_df, type = "prob")[,"1"]
  
  predictions <- ifelse(pred_probs > 0.5, 1, 0)
  
   error_rates[i] <- mean(predictions != Ytest)
  
   cross_entropies_losses[i] <- CrossEntropyLoss(pred_probs, as.numeric(Ytest))
  
  TP <- sum(predictions == 1 & Ytest == 1)
  TN <- sum(predictions == 0 & Ytest == 0)
  FP <- sum(predictions == 1 & Ytest == 0)
  FN <- sum(predictions == 0 & Ytest == 1)
  
   true_positive_rates[i] <- ifelse((TP + FN) > 0, TP / (TP + FN))
   true_negative_rates[i] <- ifelse((TN + FP) > 0, TN / (TN + FP))
}
mean_error_rate <- mean(error_rates)
  mean_cross_entropy_loss <- mean(cross_entropies_losses)
  mean_tp_rate <- mean(true_positive_rates)
  mean_tn_rate <- mean(true_negative_rates)

return( kable(data.frame(mean_error_rate,mean_cross_entropy_loss,
      mean_tn_rate, mean_tp_rate), digits = 3)
    )

}

RF(noImp1)
RF(noImp2)
RF(noImp3)
RF(Imp1)
RF(Imp2)
RF(Imp3)

```


Gradient Boosting Results
```{r}
BOOST = function(kfold_list, eta =  .3, max_depth= 6, min_child_weight= 1, 
                 subsample = 1, colsample_bytree = 1, 
                 nrounds = 10000, verbose = 0, k = 5, early_stopping_rounds = 10){
  
  # get all permutations of parameters
  boostParams = expand.grid(eta = eta, max_depth = max_depth, min_child_weight = min_child_weight, subsample = subsample, colsample_bytree = colsample_bytree,
objective = "binary:logistic")
  
  # initialize data frame for results
  results = data.frame(id = numeric(), eta = numeric(), max_depth = numeric(), 
min_child_weight = numeric(), subsample = numeric(), 
colsample_bytree = numeric(), bestIter = numeric(), CELoss = numeric(), 
accuracy = numeric(), TPR = numeric(), TNR = numeric())
  
  # loop through each row of parameters
  for(row in 1:nrow(boostParams)){
    params = as.list(boostParams[row,])
    
    # loop through each fold
    for (i in 1:k){
      
      # separate train, validate, and test sets
      Xt = as.matrix(kfold_list[[paste0("Xtrain", i)]])
      Yt = as.vector(kfold_list[[paste0("Ytrain", i)]])
      # The original train is split into train and validation with roughly 80% in train and 20% in validate
      cutoff = round(nrow(Xt)*0.8)
      c1 = cutoff+1
      Xtrain = Xt[1:cutoff,]
      Ytrain = Yt[1:cutoff]
      Xvalid = Xt[c1:nrow(Xt),]
      Yvalid = Yt[c1:nrow(Xt)]
      Xtest = as.matrix(kfold_list[[paste0("Xtest", i)]])
      Ytest = as.vector(kfold_list[[paste0("Ytest", i)]])
      
      #convert train and validate into xgb matrices
      trainXG = xgb.DMatrix(data = Xtrain, label = Ytrain)
      validXG = xgb.DMatrix(data = Xvalid, label = Yvalid)
      wl = list(train = trainXG, eval = validXG)
      
      # train with early stopping using the validation set.
      trainFit = xgb.train(params = params, data = trainXG, watchlist = wl,
                           nrounds = nrounds, verbose = verbose, 
                           early_stopping_rounds = early_stopping_rounds)
      
      # predict using the test set
      probabilities = predict(trainFit, Xtest, iteration_range = c(0,  trainFit$best_iteration))
      classes = ifelse(probabilities > 0.5, 1, 0)
      
      # calculate performance metrics
      ceL = CrossEntropyLoss(probabilities, Ytest)
      acc = mean(classes == Ytest)
      TP = sum(classes == 1 & Ytest == 1)
      TN = sum(classes == 0 & Ytest == 0)
      FP = sum(classes == 1 & Ytest == 0)
      FN = sum(classes == 0 & Ytest == 1)
      TPR = ifelse((TP + FN) > 0, TP / (TP + FN))
      TNR = ifelse((TN + FP) > 0, TN / (TN + FP))
      
      # row bind performance metrics to the data frame
      results = rbind(results, list(id = row, eta = params$eta, max_depth = params$max_depth, min_child_weight = params$min_child_weight, subsample = params$subsample, colsample = params$colsample_bytree, bestIter = trainFit$best_iteration, CELoss = ceL, accuracy = acc, TPR = TPR, TNR = TNR))
    }
  }
  return(results)
}
```


```{r}
boostnoImp1 = BOOST(noImp1, eta = c(0.01, 0.1), max_depth = c(2,6))
boostnoImp2 = BOOST(noImp2, eta = c(0.01, 0.1), max_depth = c(2,6))
boostnoImp3 = BOOST(noImp3, eta = c(0.01, 0.1), max_depth = c(2,6))
boostImp1 = BOOST(Imp1, eta = c(0.01, 0.1), max_depth = c(2,6))
boostImp2 = BOOST(Imp2, eta = c(0.01, 0.1), max_depth = c(2,6))
boostImp3 = BOOST(Imp3, eta = c(0.01, 0.1), max_depth = c(2,6))
```


Ethan - plot graphs and accuracy vs cross entropy loss
```{r}
ggplot(boostnoImp1, aes(x = eta, y = accuracy, color = as.factor(id))) + 
  geom_point() + facet_wrap(~as.factor(max_depth))
```


Bart Results
```{r}
bartt = function(kfold_list, numfolds = 5, k = 2, power = 2, base = 0.95, ntree = 200, seed = 556){
  
  bartParams = expand.grid(k = k, power = power, base = base, ntree = ntree)
  results = data.frame(id = numeric(), k = numeric(), power = numeric(), 
  base = numeric(), ntree = numeric())
  # loop through each row of parameters
  for(row in 1:nrow(bartParams)){
    params = as.list(bartParams[row,])
    
    # loop through each fold
    for (i in 1:numfolds){
      # separate train, validate, and test sets
      Xt = as.matrix(kfold_list[[paste0("Xtrain", i)]])
      Yt = as.vector(kfold_list[[paste0("Ytrain", i)]])
      # The original train is split into train and validation with roughly 80% in train and 20% in validate
      cutoff = round(nrow(Xt)*0.8)
      c1 = cutoff+1
      Xtrain = Xt[1:cutoff,]
      Ytrain = Yt[1:cutoff]
      Xvalid = Xt[c1:nrow(Xt),]
      Yvalid = Yt[c1:nrow(Xt)]
      Xtest = as.matrix(kfold_list[[paste0("Xtest", i)]])
      Ytest = as.vector(kfold_list[[paste0("Ytest", i)]])
      trainFit = bart(x.train = Xtrain, y.train = Ytrain, k = params$k,
      power = params$power, base = params$base, ntree = params$ntree,
      keeptrees = TRUE)
      # predict using the test set and validation set
      posteriorValidate = predict(trainFit, newdata = Xvalid)
      probsValidate = colMeans(posteriorValidate)
      classesValidate = ifelse(probsValidate > 0.5, 1, 0)
    
      posteriorTest = predict(trainFit, newdata = Xtest)
      probsTest = colMeans(posteriorTest)
      classesTest = ifelse(probsTest > 0.5, 1, 0)
      
      # calculate performance metrics
      ceLValidate = CrossEntropyLoss(probsValidate, Yvalid)
      ceLTest = CrossEntropyLoss(probsTest, Ytest)
      accValidate = mean(classesValidate == Yvalid)
      accTest = mean(classesTest == Ytest)
      TP = sum(classesTest == 1 & Ytest == 1)
      TN = sum(classesTest == 0 & Ytest == 0)
      FP = sum(classesTest == 1 & Ytest == 0)
      FN = sum(classesTest == 0 & Ytest == 1)
      TPR = ifelse((TP + FN) > 0, TP / (TP + FN))
      TNR = ifelse((TN + FP) > 0, TN / (TN + FP))
    
      # row bind performance metrics to the data frame
      results = rbind(results, list(id = row, k = params$k, power = params$power, 
                base = params$base, ntree = params$ntree, 
                CELossValidate = ceLValidate, CELossTest = ceLTest, 
                accValidate = accValidate, accTest = accTest, 
                TPR = TPR, TNR = TNR))
    }
  }
  return(results)
}
```

```{r, include=FALSE}
bartnoImp1 = bartt(kfold_list = noImp1, k = c(1,3,5), ntree = 200)
bartnoImp2 = bartt(kfold_list = noImp2, k = c(1,3,5), ntree = 200)
bartnoImp3 = bartt(kfold_list = noImp3, k = c(1,3,5), ntree = 200)
bartImp1 = bartt(kfold_list = Imp1, k = c(1,3,5), ntree = 200)
bartImp2 = bartt(kfold_list = Imp2, k = c(1,3,5), ntree = 200)
bartImp3 = bartt(kfold_list = Imp3, k = c(1,3,5), ntree = 200)
```


Ethan - Plot Bart
```{r}
ggplot(bartnoImp1, aes(x = k, y = accValidate)) + geom_point()
```

